# chatbot_app.py
"""
ì •ì±…ë¬¸ì„œ ê¸°ë°˜ ê³ ê° ë¶ˆë§Œ ìë™ ë¶„ë¥˜ & ëŒ€ì‘ ì±—ë´‡

â€¢ policy_docs/ í´ë”ì˜ .txt/.pdf ë¬¸ì„œë¥¼ ì½ì–´
  ë³€ê²½ ê°ì§€ ì‹œë§ˆë‹¤ FAISS ì¸ë±ìŠ¤ ë””ë ‰í„°ë¦¬ ìƒì„±/ë¡œë“œ
â€¢ LangChain APIì™€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ RAG ì²´ì¸ êµ¬ì„±
â€¢ st.chat_input / st.chat_message ë¡œ ì¹´í†¡Â·GPT ìŠ¤íƒ€ì¼ UI
â€¢ ë©”ì‹œì§€ ì§€ì—° ì—†ì´ ì¦‰ì‹œ í‘œì‹œ
"""

# 0ï¸âƒ£ ê°œë°œì ì „ìš© ì„¤ì •
TEMPERATURE   = 0.2        # ì‘ë‹µì˜ ì°½ì˜ì„± (0.0â€Šâ€“â€Š1.0)
N_CANDIDATES  = 3           # ìƒì„± ë‹µë³€ ìˆ˜
RETRIEVE_K    = 2           # ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜ â†“ ì†ë„â†‘
CHUNK_SIZE    = 400         # ì²­í¬ í¬ê¸° â†“ ì†ë„â†‘
CHUNK_OVERLAP = 40          # ì²­í¬ ì˜¤ë²„ë© â†“ ì†ë„â†‘
MODEL_NAME    = "gpt-4o-mini" # OpenAI ëª¨ë¸ ì´ë¦„

import streamlit as st
from dotenv import load_dotenv
from pathlib import Path
import hashlib

# LangChain / FAISS
from langchain.chat_models import ChatOpenAI
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.document_loaders import TextLoader
try:
    from langchain.document_loaders import PyPDFLoader
except ImportError:
    PyPDFLoader = None
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ìš©
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

# 1ï¸âƒ£ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()  # .envì— OPENAI_API_KEY í•„ìš”

# 2ï¸âƒ£ ì»¤ìŠ¤í…€ CSS ì ìš©
st.markdown(
    """
    <style>
    /* ë°°ê²½ìƒ‰ Â· í°íŠ¸ */
    body {background-color: #f4f6f8;}
    .css-18e3th9 {padding: 1rem 2rem;}  /* main container */
    /* í—¤ë” ìŠ¤íƒ€ì¼ */
    .title {font-size:2.5rem; color:#0057a4; font-weight:bold; margin-bottom:0;}
    .subtitle {font-size:1rem; color:#444; margin-top:0.2rem;}
    /* ì‚¬ì´ë“œë°” í—¤ë” */
    .sidebar .css-1d391kg h2 {color:#0057a4;}
    /* ì±„íŒ… ë°•ìŠ¤ */
    .stChatMessage > div {
        border-radius: 12px !important;
        padding: 0.75rem !important;
        font-size: 0.95rem;
    }
    /* ì‚¬ìš©ì ë©”ì‹œì§€ */
    .stChatMessage.stChatMessageUser > div {
        background-color: #e1f5fe !important;
        color: #333 !important;
    }
    /* ì±—ë´‡ ë©”ì‹œì§€ */
    .stChatMessage.stChatMessageAssistant > div {
        background-color: #ffffff !important;
        border: 1px solid #ddd !important;
    }
    /* ì…ë ¥ì°½ */
    .stChatInput>div>div>div>textarea {
        border-radius: 8px !important;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# 3ï¸âƒ£ Streamlit í˜ì´ì§€ ì„¤ì • & í—¤ë”
st.set_page_config(page_title="NCSOFT ìš´ì˜ì•½ê´€ ì±—ë´‡", layout="wide")
st.markdown('<h1 class="title">ğŸ’¬ NCSOFT ìš´ì˜ì•½ê´€ ì±—ë´‡</h1>', unsafe_allow_html=True)
st.markdown('<p class="subtitle">NCSOFT ê³µì‹ ìš´ì˜ì•½ê´€ì„ ê¸°ë°˜ìœ¼ë¡œ ê³ ê° ë¬¸ì˜ì— ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤.</p>', unsafe_allow_html=True)
st.markdown("---")

# 4ï¸âƒ£ ì‚¬ì´ë“œë°” ì•ˆë‚´
with st.sidebar:
    st.markdown('<h2>âš™ï¸ ì‚¬ìš©ë²•</h2>', unsafe_allow_html=True)
    st.markdown(
        """
        - `policy_docs/` í´ë”ì— NCSOFT ìš´ì˜ì•½ê´€(.txt/.pdf) íŒŒì¼ì„ ë„£ê³  ìƒˆë¡œê³ ì¹¨  
        - ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜: **%d**, ì²­í¬ í¬ê¸°: **%d/%d**  
        - ëª¨ë¸: **%s**, ì˜¨ë„: **%.2f**, í›„ë³´ ìˆ˜: **%d**
        """ % (RETRIEVE_K, CHUNK_SIZE, CHUNK_OVERLAP, MODEL_NAME, TEMPERATURE, N_CANDIDATES)
    )

# 4ï¸âƒ£ ë¬¸ì„œ ë³€ê²½ ê°ì§€ í•´ì‹œ
def get_docs_hash() -> str:
    docs_dir = Path(__file__).parent / "policy_docs"
    h = hashlib.md5()
    for f in sorted(docs_dir.glob("*")):
        h.update(f.name.encode())
        h.update(str(f.stat().st_mtime).encode())
    return h.hexdigest()

# 5ï¸âƒ£ FAISS ì¸ë±ìŠ¤ ìƒì„±/ë¡œë“œ
@st.cache_resource
def get_vectorstore(docs_hash: str):
    base = Path(__file__).parent
    docs_dir = base / "policy_docs"
    index_dir = base / f"faiss_index_{docs_hash}"
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    if index_dir.exists():
        return FAISS.load_local(
            str(index_dir), embeddings, allow_dangerous_deserialization=True
        )

    raw_docs = []
    for f in docs_dir.glob("*"):
        if f.suffix.lower() == ".pdf":
            if PyPDFLoader:
                loader = PyPDFLoader(str(f))
            else:
                st.warning(f"PDF ë¬´ì‹œ: {f.name}")
                continue
        else:
            loader = TextLoader(str(f), encoding="utf-8")
        raw_docs.extend(loader.load())

    splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = splitter.split_documents(raw_docs)
    vs = FAISS.from_documents(chunks, embeddings)
    vs.save_local(str(index_dir))
    return vs

# 6ï¸âƒ£ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë° ì‘ë‹µ í”„ë¡¬í”„íŠ¸ ì •ì˜
system_template = """
ë‹¹ì‹ ì€ ë‹¹ì‹ ì€ NCSOFT ê³ ê° ì§€ì› ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ë‹µë³€ ë‚´ìš©ì€ policy_docs í´ë” ë‚´ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ ì „ë‹¬í•©ë‹ˆë‹¤.
ë‹µë³€ì‹œ ì§ˆë¬¸ ë‚´ìš©ì— ëŒ€í•œ ì •ì˜ëŠ” í¬í•¨í•˜ì§€ ì•Šê³  í•´ê²° ë°©ì•ˆì— ëŒ€í•´ì„œ ì „ë‹¬í•©ë‹ˆë‹¤.
ì •ì¤‘í•œ ë¬¸ì²´ë¥¼ ì‚¬ìš©í•´ ë‹µë³€í•©ë‹ˆë‹¤.
"""

# RAGì˜ ë‹µë³€ ì¡°í•© ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  Prompt
combine_prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template("ë¬¸ì„œ ë‚´ìš©:\n{context}\n\nì§ˆë¬¸:\n{question}")
])

# 7ï¸âƒ£ RAG ì²´ì¸ ì´ˆê¸°í™”
@st.cache_resource
def init_chain(docs_hash: str):
    vs = get_vectorstore(docs_hash)
    llm = ChatOpenAI(
        model_name=MODEL_NAME,
        temperature=TEMPERATURE,
        n=N_CANDIDATES
    )
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vs.as_retriever(search_kwargs={"k": RETRIEVE_K}),
        memory=memory,
        combine_docs_chain_kwargs={"prompt": combine_prompt}
    )

# 8ï¸âƒ£ í•´ì‹œ ê³„ì‚° & RAG ì²´ì¸ ë¡œë“œ
docs_hash = get_docs_hash()
with st.spinner("â³ ì¸ë±ìŠ¤/ì²´ì¸ ì´ˆê¸°í™” ì¤‘â€¦"):
    qa_chain = init_chain(docs_hash)

# 9ï¸âƒ£ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ğŸ”Ÿ ì‚¬ìš©ì ì…ë ¥ & ì‘ë‹µ
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”â€¦")
if user_input:
    st.session_state.chat_history.append(("user", user_input))
    
    # â€”â€”â€”ë¦¬íŠ¸ë¦¬ë²„ ë””ë²„ê¹… â€”â€”â€”
    #docs = qa_chain.retriever.get_relevant_documents(user_input)
    #st.markdown(f"**[ë””ë²„ê·¸]** ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
    #for i, d in enumerate(docs, 1):
    #    st.markdown(f"- ë¬¸ì„œ #{i} (ì• 200ì): {d.page_content[:200]!r}")
    
    resp = qa_chain({"question": user_input})
    st.session_state.chat_history.append(("assistant", resp["answer"]))

# â“« ëŒ€í™” ë Œë”ë§
for speaker, msg in st.session_state.chat_history:
    if speaker == "user":
        st.chat_message("user").write(msg)
    else:
        st.chat_message("assistant").write(msg)
