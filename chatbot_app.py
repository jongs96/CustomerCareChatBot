# chatbot_app.py
"""
ì •ì±…ë¬¸ì„œ ê¸°ë°˜ ê³ ê° ë¶ˆë§Œ ìë™ ë¶„ë¥˜ & ëŒ€ì‘ ì±—ë´‡

â€¢ policy_docs/ í´ë”ì˜ .txt/.pdf ë¬¸ì„œë¥¼ ì½ì–´
  ë³€ê²½ ê°ì§€ ì‹œë§ˆë‹¤ FAISS ì¸ë±ìŠ¤ ë””ë ‰í„°ë¦¬ ìƒì„±/ë¡œë“œ
â€¢ LangChain APIë¡œ RAG ì²´ì¸ êµ¬ì„±
â€¢ st.chat_input / st.chat_message ë¡œ ì¹´í†¡Â·GPT ìŠ¤íƒ€ì¼ UI
â€¢ ë©”ì‹œì§€ ì§€ì—° ì—†ì´ ì¦‰ì‹œ í‘œì‹œ
"""

# 0ï¸âƒ£ ê°œë°œì ì „ìš© ì„¤ì • (ì´ìš©ìëŠ” UIì—ì„œ ë³€ê²½ ë¶ˆê°€)
TEMPERATURE   = 0.3   # ì‘ë‹µì˜ ì°½ì˜ì„± ì •ë„ (0.0â€Šâ€“â€Š1.0)
N_CANDIDATES  = 3     # í•œë²ˆì— ìƒì„±í•  ë‹µë³€ ìˆ˜(LLM ì´ˆê¸°í™” ì‹œ ë°˜ì˜)

import streamlit as st
from dotenv import load_dotenv
from pathlib import Path
import hashlib

# LangChain / FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.document_loaders import TextLoader
try:
    from langchain.document_loaders import PyPDFLoader
except ImportError:
    PyPDFLoader = None
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# 1ï¸âƒ£ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()  # .envì— OPENAI_API_KEY í•„ìˆ˜

# 2ï¸âƒ£ Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì •ì±…ë¬¸ì„œ ê¸°ë°˜ ê³ ê° ë¶ˆë§Œ ì±—ë´‡",
    layout="wide",
)
st.title("ğŸ’¬ ì •ì±…ë¬¸ì„œ ê¸°ë°˜ ê³ ê° ë¶ˆë§Œ ì±—ë´‡")
st.markdown("`policy_docs/` í´ë” ì•ˆì˜ .txt/.pdf ë¬¸ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìë™ ì‘ë‹µí•©ë‹ˆë‹¤.")
st.markdown("---")

# 3ï¸âƒ£ ì‚¬ì´ë“œë°” ì•ˆë‚´
with st.sidebar:
    st.header("âš™ï¸ ì‚¬ìš©ë²•")
    st.markdown(
        "- `policy_docs/` í´ë”ì— `.txt` ë˜ëŠ” `.pdf` íŒŒì¼ì„ ë„£ìœ¼ì„¸ìš”.\n"
        "- PDF ì§€ì›: `pip install pypdf` â†’ PyPDFLoader ìë™ í™œì„±í™”\n"
        "- `.env` íŒŒì¼ì— `OPENAI_API_KEY`ë¥¼ ì„¤ì •í•˜ì„¸ìš”.\n"
        "- ë¬¸ì„œ ë³€ê²½ í›„ ìƒˆë¡œê³ ì¹¨í•˜ë©´ ì¸ë±ìŠ¤ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.\n"
        "- 2íšŒì°¨ ì‹¤í–‰ë¶€í„°ëŠ” ì¦‰ì‹œ ë¡œë“œë©ë‹ˆë‹¤."
    )

# 4ï¸âƒ£ ë¬¸ì„œ ë³€ê²½ ê°ì§€ í•´ì‹œ ìƒì„±
def get_docs_hash() -> str:
    """
    policy_docs/ ë‚´ íŒŒì¼ëª…+ìˆ˜ì •ì‹œê° ë¦¬ìŠ¤íŠ¸ë¡œ MD5 í•´ì‹œ ìƒì„±.
    ë³€ê²½ ì‹œë§ˆë‹¤ í•´ì‹œê°€ ë‹¬ë¼ì ¸ ì €ì¥ ë””ë ‰í„°ë¦¬ê°€ ë¶„ê¸°ë©ë‹ˆë‹¤.
    """
    docs_dir = Path(__file__).parent / "policy_docs"
    h = hashlib.md5()
    for f in sorted(docs_dir.glob("*")):
        h.update(f.name.encode("utf-8"))
        h.update(str(f.stat().st_mtime).encode("utf-8"))
    return h.hexdigest()

# 5ï¸âƒ£ FAISS ì¸ë±ìŠ¤ ìƒì„±/ë¡œë“œ (ë””ë ‰í„°ë¦¬ ê¸°ë°˜)
@st.cache_resource
def get_vectorstore(docs_hash: str):
    """
    â€¢ ì¸ë±ìŠ¤ ë””ë ‰í„°ë¦¬: faiss_index_{docs_hash}
    â€¢ ë””ë ‰í„°ë¦¬ ì¡´ì¬ ì‹œ FAISS.load_local(..., allow_dangerous_deserialization=True) ë¡œë“œ
    â€¢ ì—†ìœ¼ë©´ ë¬¸ì„œ ë¡œë“œâ†’ì„ë² ë”©â†’FAISS.from_documentsâ†’save_local
    """
    base = Path(__file__).parent
    docs_dir = base / "policy_docs"
    index_dir = base / f"faiss_index_{docs_hash}"

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # 1) ê¸°ì¡´ ì¸ë±ìŠ¤ ë””ë ‰í„°ë¦¬ ë¡œë“œ (pickle ê²½ê³  íšŒí”¼)
    if index_dir.exists():
        return FAISS.load_local(
            str(index_dir),
            embeddings,
            allow_dangerous_deserialization=True  # âš ï¸ ì‹ ë¢°ëœ ë¡œì»¬ ë°ì´í„°ì´ë¯€ë¡œ í—ˆìš©
        )

    # 2) ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
    raw_docs = []
    for f in docs_dir.glob("*"):
        if f.suffix.lower() == ".pdf":
            if PyPDFLoader:
                loader = PyPDFLoader(str(f))
            else:
                st.warning(f"PDF ë¬´ì‹œ(PyPDFLoader ë¯¸ì„¤ì¹˜): {f.name}")
                continue
        else:
            loader = TextLoader(str(f), encoding="utf-8")
        raw_docs.extend(loader.load())

    # ë¬¸ì„œ ë¶„í•  â†’ ì„ë² ë”© â†’ FAISS ìƒì„±
    splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(raw_docs)
    vs = FAISS.from_documents(chunks, embeddings)

    # ë””ë ‰í„°ë¦¬ë¡œ ì €ì¥
    vs.save_local(str(index_dir))
    return vs

# 6ï¸âƒ£ RAG ì²´ì¸ ì´ˆê¸°í™” (LangChain API)
@st.cache_resource
def init_chain(docs_hash: str):
    vs = get_vectorstore(docs_hash)

    # LLM ì´ˆê¸°í™” ì‹œ ì˜¨ë„ ë° í›„ë³´ ìˆ˜ ê³ ì •
    llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        temperature=TEMPERATURE,
        model_kwargs={"n": N_CANDIDATES}
    )
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vs.as_retriever(search_kwargs={"k": 3}),
        memory=memory
    )

# 7ï¸âƒ£ í•´ì‹œ ê³„ì‚° & RAG ì²´ì¸ ë¡œë“œ
docs_hash = get_docs_hash()
with st.spinner("â³ ì¸ë±ìŠ¤ ìƒì„±/ë¡œë“œ ë° RAG ì´ˆê¸°í™” ì¤‘â€¦ ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"):
    qa_chain = init_chain(docs_hash)

# 8ï¸âƒ£ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []  # List of (speaker, message)

# 9ï¸âƒ£ ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”â€¦")
if user_input:
    st.session_state.chat_history.append(("user", user_input))
    resp = qa_chain({"question": user_input})
    st.session_state.chat_history.append(("assistant", resp["answer"]))

# ğŸ”Ÿ ëŒ€í™” ë Œë”ë§ (ì¦‰ì‹œ í‘œì‹œ)
for speaker, msg in st.session_state.chat_history:
    if speaker == "user":
        st.chat_message("user").write(msg)
    else:
        st.chat_message("assistant").write(msg)
